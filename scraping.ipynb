{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the issue pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anaconda python 3.7.6\n",
    "from time import sleep\n",
    "from selenium import webdriver # version 3.141.0\n",
    "import pandas as pd # version 1.0.1\n",
    "from bs4 import BeautifulSoup # version 4.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make the list of pages to scrape\n",
    "page_lst = ['https://github.com/uchicago-computation-workshop/Fall2019/issues/' + str(i) for i in [1, 3, 7, 8, 9, 10]]\n",
    "page_lst += ['https://github.com/uchicago-computation-workshop/Winter2020/issues/' + str(i) for i in range(1, 8)]\n",
    "page_lst += ['https://github.com/uchicago-computation-workshop/Spring2020/issues/' + str(i) for i in range(1, 7)]\n",
    "# make the list of workshop dates\n",
    "date_lst = [\"2019-10-10\", \"2019-10-17\", \"2019-10-24\", \"2019-10-31\", \"2019-11-07\", \"2019-11-14\", \"2020-01-09\", \n",
    "            \"2020-01-16\", \"2020-01-23\", \"2020-01-30\", \"2020-02-06\", \"2020-02-27\", \"2020-03-06\", \"2020-04-09\",\n",
    "            \"2020-04-30\", \"2020-05-07\", \"2020-05-14\", \"2020-05-21\", \"2020-05-28\"]\n",
    "# sanity check\n",
    "len(page_lst) == len(date_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_github_issues(page_lst, date_lst):\n",
    "    # initialize the webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "    # intialize an empty dataframe to hold the data\n",
    "    df = pd.DataFrame()\n",
    "    # loop thorough pages and scrape\n",
    "    for idx, url in enumerate(page_lst):\n",
    "        tmp = scrape_github_issue(url, driver, date_lst[idx])\n",
    "        df = df.append(tmp)\n",
    "    driver.close()\n",
    "    return df\n",
    "\n",
    "def scrape_github_issue(url, driver, date):\n",
    "    # load the url and click on the \"load more\" button to get the full list of comments\n",
    "    driver.get(url)\n",
    "    driver.find_element_by_class_name('ajax-pagination-btn').click()\n",
    "    # wait for the page to load: kind of hacky but also prevents DDOS attacking GitHub\n",
    "    sleep(4)\n",
    "    # use beautifulsoup to get the html\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    # parse the fist element because it is the preceptor\n",
    "    all_comments = soup.find_all(\"div\", class_=\"ml-n3 timeline-comment unminimized-comment comment previewable-edit js-task-list-container js-comment timeline-comment--caret\")[1:]\n",
    "    # create a list of dictionaries holding the scraped stuff\n",
    "    lst = make_lst(all_comments, date)\n",
    "    return pd.DataFrame(lst)\n",
    "\n",
    "def make_lst(all_comments, date):\n",
    "    # initialize empty list\n",
    "    lst = []\n",
    "    # loop through results from find_all and extract the data, put it in a dictionary and append to the list\n",
    "    for idx, comment in enumerate(all_comments):\n",
    "        dict = {}\n",
    "        dict[\"name\"] = comment.find(\"a\", class_=\"author link-gray-dark css-truncate-target width-fit\").text\n",
    "        dict[\"time\"] = comment.find(\"a\", class_=\"link-gray js-timestamp\").find(\"relative-time\")[\"datetime\"]\n",
    "        dict[\"workshop_date\"] = date\n",
    "        dict[\"position\"] = idx\n",
    "        dict[\"text\"] = comment.find(\"td\", \"d-block comment-body markdown-body js-comment-body\").text.replace(\"\\n\", \" \").strip()\n",
    "        temp_upvote = comment.find(\"div\", class_=\"comment-reactions-options\")\n",
    "        # if the comment got no upvote such element would not exist\n",
    "        if temp_upvote is None:\n",
    "            dict[\"num_upvotes\"] = 0\n",
    "        else:\n",
    "            dict[\"num_upvotes\"] = int(temp_upvote.text.split()[1])\n",
    "        lst.append(dict)\n",
    "    return lst\n",
    "\n",
    "def get_time_to_deadline(row):\n",
    "    # because of the summer time which I still do not understand I need a separate function for this\n",
    "    # ended at November 3rd\n",
    "    if row['workshop_date'] in [\"2019-10-10\", \"2019-10-17\", \"2019-10-24\", \"2019-10-31\", \"2020-04-09\",\n",
    "                               \"2020-04-30\", \"2020-05-07\", \"2020-05-14\", \"2020-05-21\", \"2020-05-28\"]:\n",
    "        return pd.to_datetime(row['time']) - pd.to_datetime(row['workshop_date'] + '-06', utc=True)\n",
    "    else:\n",
    "        return pd.to_datetime(row['time']) - pd.to_datetime(row['workshop_date'] + '-05', utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scrape_github_issues(page_lst, date_lst)\n",
    "df['time_to_deadline'] = df.apply(get_time_to_deadline, axis=1).astype('timedelta64[m]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('scraped_data.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
